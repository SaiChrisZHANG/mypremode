\section{Discussion}
    
    \frame{\sectionpage}
    
    \begin{frame}{Literature: Continuity-Based RD}
        
            Most popular: local linear regression \citep{hahn2001identification,imbens2008regression}
        $$
        \hat{\tau}_{c}=\arg\min_{\tau}\left\{ \sum_{i=1}^{n}\textcolor<2->{mygreen}{\underbrace{K}_{\text{weighting}}}\left(\frac{\left|Z_{i}-c\right|}{\textcolor<2->{mygreen}{\underbrace{h_{n}}_{\text{bandwidth}}}}\right)\left(Y_{i}-a-\tau W_{i}-\beta_{-}\left(Z_{i}-c\right)_{-}-\beta_{+}\left(Z_{i}-c\right)_{+}\right)^{2}\right\} 
        $$
        
        \uncover<3->{
            \begin{itemize}
                \item $\mu_{(w)}(z)=\mathbb{E}\left[Y(w)\mid Z=z\right] $ is \textcolor{mygreen}{\textbf{smooth}}
                \item $h_n$ decays at an \textcolor{mygreen}{\textbf{appropriate}} rate
            \end{itemize}
        }

        \uncover<4->{Robust CIs {\footnotesize\citep{armstrong2020simple,calonico2014robust,kolesar2018inference}}}; \uncover<5->{Data-adaptive bandwidths {\footnotesize \citep{imbens2012optimal}}}
        
    \end{frame}

    \begin{frame}{Literature: Continuity-Based RD extended}
        
    $$
    \mu_{(w)}(z)=\mathbb{E}\left[Y(w)\mid Z=z\right]
    $$
    If further assume \textcolor{mygreen}{\textbf{convexity}} of $\mu_{(w)}(z)$, e.g.:
    $$
        \left\vert \mu_{(w)}''(z) \right\vert \leq B, \forall z\in\mathbb{R}
    $$
    \uncover<2->{\textcolor{mygreen}{\textbf{\underline{Optimization-based RD}}}: the treatment effect $\tau_c$ can be estimated {\footnotesize (minimax linear estimation)} via \textcolor{mygreen}{\textbf{numerical convex optimization}} {\footnotesize \citep{armstrong2018optimal,imbens2019optimized}}
    }
    
\end{frame}

\begin{frame}{Link Noise-Induced RD and Continuity-Based RD}
    \begin{align*}
        \mu_{\left(w\right)}\left(z\right)&=\mathbb{E}\left[Y_i\left(w\right)\mid Z_i=z\right]\\
        & =\frac{\int\mathbb{E}\left[Y_i\left(w\right)\mid U_i=u,Z_i=z\right]p\left(z\mid u\right)\mathrm{d}G\left(u\right)}{f_{G}\left(z\right)} = \frac{\int \only<1>{\alpha_{\left(w\right)}\left(u\right)} \only<2->{\boxed{\alpha_{\left(w\right)}\left(u\right)}} \textcolor<2->{mygreen}{p\left(z\mid u\right)} \mathrm{d}G\left(u\right)}{\int \textcolor<2->{mygreen}{p\left(z\mid u\right)}\mathrm{d}G\left(u\right)}
    \end{align*}

    \only<3-4>{
        \small
        \textcolor{mygreen}{\textbf{Convexity}} assumption on $\mu_{(w)}(z)$:
    $$
        \left\vert \mu_{(w)}''(z) \right\vert \leq B, \forall z\in\mathbb{R}
    $$
    }

    \uncover<4->{
        \normalsize
    Then the worst-case possible curvature is:
    $$
    \mathrm{Curv}\left(z,\rho,p\right)=\sup\left\{ \left|\frac{\mathrm{d}^{2}\mu_{\left(w\right)}\left(z\right)}{\mathrm{d}z^{2}}\right|:f_{G}\left(z\right)=\int p\left(z\mid u\right)\mathrm{d}G\left(u\right)\textcolor{mygreen}{\ge\rho>0},\alpha_{\left(w\right)}\left(\cdot\right)\in\left[0,1\right]\right\} 
    $$}

    \only<5->{
        \citet{armstrong2020simple}: fit 4th-degree polynomials to $\mu_{(0)}(z)$ and $\mu_{(1)}(z)$, and take the largest estimated curvature obtained anywhere
        \vspace*{10pt}
    }
    
\end{frame}

\begin{frame}{Literature: Randomization Inference RD}
    Posit a non-trivial interval $\mathcal{I}$ with $c\in\mathcal{I}$ s.t.
    $$
    \left\{ Y_{i}\left(0\right),Y_{i}\left(1\right)\right\} \perp Z_i \mid \left\{ Z_{i}\in\mathcal{I}\right\}
    $$
    then focus on this interval, perform classical \textcolor{mygreen}{\textbf{randomized}} study inference

    
        \begin{itemize}
            \item<2-> Design-based approach {\footnotesize\citep{rubin2008objective}}
            \item<3-> Strong assumption
            
            No \textcolor{mygreen}{\textbf{data-driven way}} of choosing $\mathcal{I}$
                    
            If the interval $\mathcal{I}$ is known a-priori, the problem collapses to a \textcolor{mygreen}{\textbf{RCT}}
        \end{itemize}
    
\end{frame}

\begin{frame}{Measurement Error Induced RD}
    \uncover<1->{
        \citet{rokkanen2015exam} considers a similar approach, assuming:
        \begin{itemize}
            \item<2-> noisy running variables \textcolor{mygreen}{\textbf{(A2)}} and exogeneity of the noise \textcolor{mygreen}{\textbf{(A3)}}
            \item<2-> \textcolor{mygreen}{\textbf{NOT}} assuming prior knowledge of the noise distribution $p(\cdot \mid u)$
            \item<3-> A stronger assumption: observing at least 3 noisy measurements of the latent variable $U_i$, $\left\{ Z_i, \textcolor<4->{mygreen}{Z_i',Z_i''} \right\}$ 
            \uncover<5->{\begin{itemize}
                \item[-] $(U_i,Z_i,Z_i',Z_i'')$ is \textcolor{mygreen}{\textbf{joint normal}} 
                \item[-] $\alpha_{(w)}(u) = \mathbb{E}\left[ Y_i(w)\mid U_i=u\right]$ is \textcolor{mygreen}{\textbf{linear}} w.r.t. $u$
            \end{itemize}}
            
        \end{itemize}
    }
\end{frame}

\begin{frame}{RD with Ordinal Running Variables}
     Similarly, ordinal $Z_i$ {\footnotesize(bond rating, custody security score, etc.)} can be seen as a noisy measurement of a latent variable $U_i$.

     \citet{li2021regression} assume 
     $$
        U_i = \mathbf{X}_i \mathbf{\beta}
     $$
     then use \textcolor{mygreen}{\textbf{inverse-propensity weighting}} with estimated propensities $e(u)=\mathbb{P}\left[ Z_i\geq c\mid U_i=u \right]$ for inference.


     \uncover<2->{\vspace*{20pt}\textcolor{mygreen}{\textbf{Assuming}}: $U_i$ can be observed, and well predicted by $\mathbf{X}_i$}
\end{frame}

\begin{frame}{Measurement Errors}
    \begin{itemize}
        \item The \textcolor{mygreen}{\textbf{running variable}} is unobserved, only a noisy measurement is observed
        
        \citet{bartalotti2021correction,davezies2017regression,dong2021can,pei2017devil}
        \item Measurement error in causal inference beyond RD
        
        \citet{pearl2012measurement,kuroki2014measurement,jiang2020measurement}
    \end{itemize}
    
\end{frame}

\begin{frame}{A Comparison}
    \begin{table}[h!]
    \small
    \begin{center}
        \label{tab:summary}
        \begin{tabular}{rl}
        
        RD designs & Assumptions\\
        \hline
        & \\
        Noise-induced RD & a \textcolor<1-2>{mygreen}{\textbf{known distribution}} of the measurement error $p(\cdot\mid u)$\\
        Noise-induced RD {\footnotesize\citep{rokkanen2015exam}} & \textcolor<1>{mygreen}{\textbf{multiple joint-normal}} noisy measurements $(U_i,Z_i,Z_i',Z_i'')$\\
        & \textcolor<1>{mygreen}{\textbf{linear}} $\alpha_{(w)}(u) = \mathbb{E}\left[ Y_i(w)\mid U_i=u\right]$ \\
        & \\
        \hline
        & \\
        Continuity-based RD & $\mu_{(w)}=\mathbb{E}\left[Y(w)\mid Z=z\right]$ is \textcolor<1>{mygreen}{\textbf{smooth}}\\
        OPtimization-based RD & \textcolor<1>{mygreen}{\textbf{convexity}} of $\mu_{(w)}(z)$: $ \left\vert \mu_{(w)}''(z) \right\vert \leq B, \forall z\in\mathbb{R}$ \\
        Randomization inference RD & an \textcolor<1>{mygreen}{\textbf{"RCT"}} interval $\mathcal{I}$: $\left\{ Y_{i}\left(0\right),Y_{i}\left(1\right)\right\} \perp Z_i \mid \left\{ Z_{i}\in\mathcal{I}\right\}$ \\
        RD with ordinal $Z_i$ & $U_i$ can be observed, and well predicted by $\mathbf{X}_i$
        \end{tabular}
    \end{center}
    \end{table}
\end{frame}